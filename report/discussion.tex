\section{Discussion}
Our aim in this project was to implement a twitter bot using genetic algorithm to generate tweet structures which would then be filled in with the help of Tracery. We selected video games as our domain for generating tweets, and used video game ratings to select an adjective which could better describe the bot's opinion about the video game. We tried to generate tweet structures which have at least one adjective and exactly one proper noun. The proper noun was filled in by the video game's name and the adjective was filled in by an adjective representing that bot's opinion about the video game.
\paragraph{}
We ran our genetic algorithm for different settings of goal parameters and came up with some optimal tweet structures, where the fitnees of the tweet structure depended on the similarity of the structure to an existing tweet with a high LR score. The tweet structures we obtained often looked syntactically sound and hence we think that our fitness function was a good measure of how a tweet should look. However filling it up with words to generate a meaningful sentence proved to be a difficult task. More often than not, the tweets failed to convey any clear message, despite some smaller parts of the tweets making some semblance of meaning. We attribute the problem with generating semantically sound tweets to the following:
\begin{enumerate}
    \item Nature of Tweets
    \item Our Domain
    \item Parts of Speech Tagging
\end{enumerate}
We discuss below how, and the extent to which these factors influenced the results.

\paragraph{Nature of Tweets}
On analyzing the generated tweet structures and manually observing tweets on twitter, we found that most of the tweets are very unstructured and contain a lot of nouns, without the presence of a lot of other parts of speech. As twitter allows a maximum size of \textit{140 characters} for any tweet, people often do not stick to grammatical rules of the language, and thus the generated tweets are more often than not unstructured and hence are not well formed sentences. On generating tweets ourselves, we were focused on generating well formed sentences, which led to a mismatch between the way people form tweets and the way we wanted to form them. Further, tweets often contain references using '\#', '@', etc. These stand out as separate tags while using a parts of speech tagger, and we decided to ignore these and retain the following noun, since people often use '\#' with nouns, while tweeting even a well formed sentenced. But more often than not, tweets are just composed of '\#' followed by nouns, with no sense of being a well formed sentence. This leads to a list containing just nouns, with no verbs, adjectives or connectors to provide any meaning to it. When filled in with the correct nouns (like humans possess the ability to do), these sentences make sense, but as we were aiming at forming well formed sentences, our approach had a rather big collision with the ideology of humans while authoring tweets. We believe that there is much more to tweet generation, than just well formed sentence, and to properly learn the art of automating tweet generation, these things have to be accounted for.

\paragraph{Our Domain}
We had chosen our domain for tweet generations as video games, and hoped to generate well formed sentences that express the bot's opinion about a video game in the tweet it generates. As explained before, most of the generated tweets contained a lot of nouns, and for us the main problem was that nouns that were in the video game domain did not make much sense when used to fill in the tweet structures randomly. In order to generate a sensible tweet, there needed to be a connection between which words are selected and the manner in which they are used within the tweet. However, as the word filling part was hand authored (except for the proper noun and adjectives), it led to poorly generated tweets which did not convey any clear meaning. We felt that there was a huge mismatch in the word list we had and the tweet structures which were being generated, despite the words in the word list being related to the domain of video games. In order to get a big enough dataset to learn from, we streamed generic tweets, and then tried to fit them in our domain, which led to generation of meaningless tweets. Perhaps if we could have had a big enough domain specific dataset, we could have hoped for better results.

\paragraph{Parts of Speech Tagging}
Now-a-days, its a trend to use shorthand and mixed up words on social media. Words from urban dictionary are being used more and more, giving rise to semantically chaotic structures of sentences, when filtered with a POS tagger. Filling in the same type of, but different words often leads to a very different and possibly meaningless sentence. Further, as parts of speech tagging detects shorthands as nouns, it adds to the increased number of nouns in a sentence, further adding to the problem of meaningless tweets being generated.

\paragraph{}
We feel that while our tweet structures may have been optimized by using the genetic algorithm, it is equally important to find an optimal way to fill in words for the corresponding parts of speech tags, and that was where our bot failed.