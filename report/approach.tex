\section{Approach}
We have divided our system and the entire task of tweet generation into 2 major parts - the data collection and machine learning part, and the tweet generation part. The idea behind this separation is to allow the use of machine learning in an offline environment, and then use its output to help generate a sentence that can be posted as a tweet on Twitter.
\subsection{Machine Learning}
This part of our system is responsible for the collection and analysis of data. The goal here is to be able to have the system learn a structure for sentences in English, that can be used to generate tweets, and have this structure be stored in a database as a grammar. This is an offline process that fetches existing tweets and uses a genetic algorithm to optimize the structure of a tweet.
\subsubsection{Data set}
All data-driven approaches require a data set to serve as an input. For our system, we streamed tweets that are posted on Twitter for a period of 2 days, to build a database that consisted of information about approximately \textit{1.5 million} tweets. This information consists of the tweet's id, the author's id and a list of parts-of-speech (\textit{POS}) tags that make up the tweet. This list of POS tags are obtained by passing the tweet's text through a POS tagger available through the \textit{NLTK Project}. The main reason behind this was that we are interested in the structure of each tweet, and not the actual content that was posted. Therefore, to extract the structure of the tweet, we decided to tag it to identify the part-of-speech that were used by it. 
\paragraph{}
Although this database contains a lot of useful information, it also contains a lot of noise. It was assumed that this database would contain tweets that would be posted by other bots and contain other tweets that would not really be seen by many people due to authors not having a sufficient public reach. Such noisy tweets were filtered out on the basis of the tweet author's follower count.
\paragraph{}
Tweets that are posted by users having too few followers are considered to not reach enough people. Hence, their popularity and insight into the structure of a "good" tweet, will also be low. We remove such tweets by storing only those tweets whose authors have more than a particular number of followers, i.e., we set a threshold on the number of followers that an author must have, for his/her tweet to be considered by our system. This reduced our data set by a considerable amount, pruning it to approximately \textit{10,000 tweets}, which is a much more manageable number.
\paragraph{}
Since the idea behind the entire learning part of our system is that popular tweets must possess some trait that facilitates their popularity, we must compute some kind of measure of the popularity of each tweet that we analyze. We characterize popularity as the tweet's LR score - a sum of the number of times the tweet was favorite (liked) and the number of times the tweet was retweeted by other users. This sum is then normalized to the range $[0, 1]$. However, there were found to be a large number of tweets that ended up with an LR score of 0. Since such an LR score would effectively remove all influence of such tweets (despite the fact that they may be well-formed and meaningful tweets, but simply did not get enough time to be liked or retweeted by others), we add a constant scale up value to both the numerator and the denominator when normalizing the LR scores. This constant was decided to be half the maximum LR score achieved by any tweet within the database of tweets.
%\includegraphics{DataCollection}
\subsubsection{Genetic Algorithm}
Once the data has been collected and annotated using the LR score, we may now run a machine learning algorithm on it. The machine learning technique used by this system is a Genetic Algorithm. We define a population of individuals that are evolved, generation-by-generation, in hopes of finding a solution that is fit enough. Here, we define our population as a selection of random tweet information, from our database of filtered tweets that was the output of the data collection process. Here, each individual from the population denotes a list of POS tags. We also define the genetic algorithm operators of fitness evaluation, selection, cross over and mutation, as follows:
\begin{description}
\item [Fitness evaluation] Each individual is evaluated based on the popularity of the kind of structure that the individual represents. Since the individual itself would not possess any LR score (it may be the result of a cross over or mutation operation from a previous generation), we compute the fitness for it by comparing its list of POS tags with that of some goal individual using a \textit{64-bit simhash} function and computing the Hamming distance on the simhash. This value is normalized by dividing it by \textit{64} (the maximum value that can be produced by the simhash), and then multiplying it with the LR score of the goal individual that it was being compared to. The idea here is that the fitness will be higher for an individual that is similar to a tweet that had a high popularity score, and lower for structures that are either dissimilar to goal individuals, or similar to goal individuals that have low popularity, or both. 
\linebreak
The goal population is made up of a collection of random collection of tweets from the database. The size of this goal population is a parameter to the system and can be modified from outside the system.
\item[Selection] The selection operation selects a particular portion of the population's fittest individuals. The operator simply sorts the population on the basis of each individual's fitness value in descending order, and then returns the top portion of the population, the \textit{elites}. The number of elites returned by this operation is a parameter to the system and can be modified from outside the system. The individuals returned by this operation are retained as is till the next generation. Also, other operators (Cross over and Mutation) are applied only on copies of some elites that are returned by this operation. The purpose of doing this is that the fittest individuals in a generation should always be retained for the next generation.
\item[Cross over] The cross over operator accepts 2 input individuals, which are considered as parents for the creation of 2 individuals for the next generation of the population. The purpose of this operator is to improve the fitness for the population. If a cross over is to take place between 2 parents, a partitioning index is chosen randomly. This partitioning index lies in the range, $[0, min(child1.length, child2.length)]$. Once the partitioning index is selected, the 2 parents are split into 2 parts each, at the partitioning index. These parts are crossed over and combined, i.e., part 1 of the parent 1 is combined with part 2 of parent 2 and the part 1 of the parent 2 is combined with part 2 of parent 1, to generate 2 children for the next generation.
\linebreak
Cross overs are not guaranteed to take place on every pair of parents. This operator is only executed with a probability, the cross over probability, which is a parameter to the system and can be modified from outside the system. In case the cross over probability is not satisfied, the parent individuals are merely duplicated for the next generation.
\item[Mutation] The mutation operator accepts 1 input individual and modifies it, in an attempt to prevent getting stuck in a local fitness maxima. If a mutation is to take place, 2 tags are chosen randomly from the individual's POS tag list. These 2 tags are then swapped amongst each other. For example, if the individual has a tag list of: [$"NN"$, $"JJ"$, $"NNP"$, $"VB"$, $"JJ"$] and the 2 randomly chosen tags are $"NN"$ and $"VB"$, the output of the mutation operation will be: [$"VB"$, $"JJ"$, $"NNP"$, $"NN"$, $"JJ"$]. This individual will then be evaluated for fitness in the next generation, and depending on that fitness value, the mutated individual may or may not be retained within the population.
\end{description}
The population of tweets is iterated a number of time (controlled through another parameter to the system) or until we get an individual that possess a higher fitness value than a threshold fitness value (again, a parameter to the system). The output of the genetic algorithm is an individual which possesses a list of POS tags that is most similar to some tweets that possesses a high popularity.
\subsubsection{Syntax Generation}
Once the system has learned an optimal tweet structure (list of POS tags), this structure must be converted into a format that can be used as input to other part of the system, the tweet grammar. The Syntax Generator is responsible for 2 main tasks:
\begin{description}
\item [Conversion into Tracery syntax]
The list of POS tags must be converted into the syntax that Tracery (the string grammar library) understands. This involves creating symbols for the various POS tags that may be present in the genetic algorithm's output structure.
\item [Applying modifiers]
Certain POS tags and their relative ordering can be better represented using certain features that Tracery provides, such as modifiers. For example, the POS tag $"VBD"$ (verb, past tense) can also be represented in Tracery as $"\#VB.ed\#"$ (a symbol for a verb, with a modifier for "ed"), the POS tag for $"NNS"$ (noun, plural) can also be represented in Tracery as $"\#NN.s\#"$ (a symbol for a noun, with a modifier for "s"). Such modifiers help decrease the number of symbols present in the grammar, thereby reducing the number of symbol replacement rules that need to be specified manually.
\item [Handling comparatives and superlatives]
%\linebreak
Certain POS tags (such as adjectives, $JJ$, and adverbs, $RB$) can have comparative and superlative forms. In order to further reduce the number of symbols present in the grammar, these tags are replaced with their normal forms suffixed with $er$ or $est$ for comparative and superlative tags, respectively.
\end{description}
\subsection{Tweet Generation}
This part of our system is responsible for the use of the grammar (learned in the previous part) to create a tweet, and subsequently, post that tweet. While the machine learning part of the system was independent of the content of the tweets (only dealing with the structure, not the matter), this part decides what will replace the symbols in the learned grammar, and what the final tweet will look like and represent. The various pieces of this part of the system are described below.
\subsubsection{Game Selector}
This piece is responsible for providing information relevant to the domain that the bot is interested in, video games. Our initial plan was to fetch information regarding different video games from websites such as IGN. However, we ended up simply mocking this data, in a database, since even a simple representation of this information suffices to demonstrate that the bot's content is restricted to only its chosen domain (video games).
\paragraph{}
The information maintained for each game is the game's title and its rating, as was given on IGN.com in the month of September 2016 (this information was merely referred and used in the database, nor parsed or fetched through any program). 
\paragraph{}
When the Tweet Generator is run, the system runs a search on Twitter for each game's title (used as a keyword). The system then computes the reachability score for each game. This is defined as the sum of the count of followers of the authors of the top 100 posts about the game (Twitter's search returns the posts with a page size of 100 posts, and we restrict ourselves to 1 page). If the reachability score for a game is higher than a threshold reachability value, it is considered a candidate that may be tweeted about. This threshold reachability value is a parameter to the system. Once a list of candidate games has been created, we pick a random game to tweet about.
\subsubsection{Sentence Generator}
Once a game has been selected, we can proceed to the generation of a sentence to tweet. For a sentence to be generated, we need 3 things: a game to tweet about, an opinion to voice and a sentence structure to follow while voicing the opinion. At this point we know the game to be tweeted about, and we have a sentence structure to voice an opinion in (the output of the machine learning part). We must now choose an opinion to voice.
\paragraph{}
The bot's opinions are decided by the ratings given to each game in the database. The system uses the rating and compares it to certain threshold values for opinions. For example, a rating higher than 9 (out of 10) means that the bot thinks the game is excellent, a rating less than 5 (out of 10) means that the bot this the game is pathetic.
\paragraph{}
The bot is provided with separate lists of adjectives for each kind of opinion that it can have for a game. Depending on the opinion the bot has about the game, a particular list is chosen. This list defines the symbol rewrite rule for adjectives ($"\#JJ\#"$) in the learned structure of the grammar.
\paragraph{}
Once we have all 3 pieces of information (the game, an opinion about the game and the structure of the sentence to voice that opinion), we pass these values to Tracery, which applies the appropriate rule rewrites to generate a sentence. This sentence is posted to Twitter.